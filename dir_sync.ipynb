{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Marco3010/dir-sync/blob/main/dir_sync.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "79ef8067",
      "metadata": {
        "id": "79ef8067"
      },
      "outputs": [],
      "source": [
        "DIR_SYNC = \"dir-sync\"\n",
        "ANALYZER = \"analyzer\"\n",
        "COLLECTOR = \"collector\""
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b671df3c",
      "metadata": {
        "id": "b671df3c"
      },
      "source": [
        "# Logging"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a5a26013",
      "metadata": {
        "id": "a5a26013"
      },
      "outputs": [],
      "source": [
        "import gzip\n",
        "import logging\n",
        "import os\n",
        "import sys\n",
        "from logging.handlers import RotatingFileHandler\n",
        "from typing import ClassVar"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b2a65465",
      "metadata": {
        "id": "b2a65465"
      },
      "outputs": [],
      "source": [
        "class ColoredFormatter(logging.Formatter):\n",
        "    \"\"\"Formatter to add colours to logs in the console.\n",
        "\n",
        "    Notes:\n",
        "        - Colors are applied only to the console handler you attach this formatter to.\n",
        "\n",
        "    \"\"\"\n",
        "\n",
        "    COLORS: ClassVar = {\n",
        "        \"DEBUG\": \"\\033[94m\",  # Blue\n",
        "        \"INFO\": \"\\033[92m\",  # Green\n",
        "        \"WARNING\": \"\\033[93m\",  # Yellow\n",
        "        \"ERROR\": \"\\033[91m\",  # Red\n",
        "        \"CRITICAL\": \"\\033[91m\\033[1m\",  # Bold Red\n",
        "    }\n",
        "    RESET = \"\\033[0m\"\n",
        "\n",
        "    def format(self, record: logging.LogRecord) -> str:\n",
        "        \"\"\"Build the colored log line.\n",
        "\n",
        "        Args:\n",
        "            record: The LogRecord produced by the logging system.\n",
        "\n",
        "        Returns:\n",
        "            The final colored string (or uncolored if no match).\n",
        "\n",
        "        \"\"\"\n",
        "        if record.levelname in self.COLORS:\n",
        "            record.levelname = f\"{self.COLORS[record.levelname]}{record.levelname}{self.RESET}\"\n",
        "        return super().format(record)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "80b517d4",
      "metadata": {
        "id": "80b517d4"
      },
      "outputs": [],
      "source": [
        "def namer(name) -> str:\n",
        "    \"\"\"Add .gz extension.\n",
        "\n",
        "    Args:\n",
        "        name (str): The name of the file.\n",
        "\n",
        "    Returns:\n",
        "        str: The name of the file with the .gz extension.\n",
        "\n",
        "    \"\"\"\n",
        "    return name + \".gz\"\n",
        "\n",
        "\n",
        "def rotator(source, dest) -> None:\n",
        "    \"\"\"Compress in gzip format.\n",
        "\n",
        "    Args:\n",
        "        source (str): The source file path.\n",
        "        dest (str): The destination file path.\n",
        "\n",
        "    \"\"\"\n",
        "    with open(source, \"rb\") as f_in, gzip.open(dest, \"wb\") as f_out:\n",
        "        shutil.copyfileobj(f_in, f_out)\n",
        "    os.remove(source)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "646df9a9",
      "metadata": {
        "id": "646df9a9"
      },
      "outputs": [],
      "source": [
        "def setup_logging(log_dir: str = \"logs\", level: int = logging.INFO) -> None:\n",
        "    \"\"\"\n",
        "    Set logging configuration.\n",
        "\n",
        "    In case of error, it returns to a basic configuration.\n",
        "\n",
        "    Args:\n",
        "        log_dir (str): The directory where logs will be stored. Defaults to \"logs\".\n",
        "        level (int): The logging level. Defaults to logging.DEBUG.\n",
        "\n",
        "    Raises:\n",
        "        Exception: If an error occurs during logging configuration.\n",
        "\n",
        "    \"\"\"\n",
        "    os.makedirs(log_dir, exist_ok=True)\n",
        "    try:\n",
        "        # Loggers\n",
        "        dir_sync_logger = logging.getLogger(DIR_SYNC)\n",
        "        dir_sync_logger.setLevel(level)\n",
        "        dir_sync_logger.propagate = False\n",
        "        analyzer_logger = logging.getLogger(ANALYZER)\n",
        "        analyzer_logger.setLevel(level)\n",
        "        analyzer_logger.propagate = False\n",
        "        collector_logger = logging.getLogger(COLLECTOR)\n",
        "        collector_logger.setLevel(level)\n",
        "        collector_logger.propagate = False\n",
        "\n",
        "        # Formatters\n",
        "        base_formatter = logging.Formatter(\n",
        "            \"%(asctime)s - %(levelname)s - [%(processName)s - %(threadName)s | %(name)s - %(message)s]\",\n",
        "            datefmt=\"%Y-%m-%d %H:%M:%S\",\n",
        "        )\n",
        "        colored_format = ColoredFormatter(\n",
        "            \"%(asctime)s - %(levelname)s - [%(processName)s - %(threadName)s] | %(name)s - %(message)s]\",\n",
        "            datefmt=\"%Y-%m-%d %H:%M:%S\",\n",
        "        )\n",
        "\n",
        "        # Handlers\n",
        "        file_handler = RotatingFileHandler(\n",
        "            log_dir + \"/dir-sync.log\", backupCount=5, maxBytes=1000000\n",
        "        )\n",
        "        file_handler.rotator = rotator\n",
        "        file_handler.namer = namer\n",
        "        file_handler.setFormatter(base_formatter)\n",
        "\n",
        "        analyzer_file_handler = RotatingFileHandler(\n",
        "            log_dir + \"/dir-sync_analyzer.log\", backupCount=5, maxBytes=1000000\n",
        "        )\n",
        "        analyzer_file_handler.rotator = rotator\n",
        "        analyzer_file_handler.namer = namer\n",
        "        analyzer_file_handler.setFormatter(base_formatter)\n",
        "\n",
        "        collector_file_handler = RotatingFileHandler(\n",
        "            log_dir + \"/dir-sync_collector.log\", backupCount=5, maxBytes=1000000\n",
        "        )\n",
        "        collector_file_handler.rotator = rotator\n",
        "        collector_file_handler.namer = namer\n",
        "        collector_file_handler.setFormatter(base_formatter)\n",
        "\n",
        "        console_handler = logging.StreamHandler(sys.stdout)\n",
        "        console_handler.setFormatter(colored_format)\n",
        "\n",
        "        if dir_sync_logger.hasHandlers():\n",
        "            dir_sync_logger.handlers.clear()\n",
        "        if analyzer_logger.hasHandlers():\n",
        "            analyzer_logger.handlers.clear()\n",
        "        if collector_logger.hasHandlers():\n",
        "            collector_logger.handlers.clear()\n",
        "        if level != logging.DEBUG:\n",
        "            dir_sync_logger.addHandler(console_handler)\n",
        "        dir_sync_logger.addHandler(file_handler)\n",
        "        analyzer_logger.addHandler(analyzer_file_handler)\n",
        "        collector_logger.addHandler(collector_file_handler)\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error setting up logging: {e}\")\n",
        "        logging.basicConfig(level=level)\n",
        "        logging.error(\"Impossible to set up logging configuration. Using default configuration.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fbe6c657",
      "metadata": {
        "id": "fbe6c657"
      },
      "source": [
        "# Decorators"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "acc85598",
      "metadata": {
        "id": "acc85598"
      },
      "outputs": [],
      "source": [
        "from collections.abc import Callable\n",
        "from functools import wraps\n",
        "from typing import ParamSpec, TypeVar"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8e5b50ac",
      "metadata": {
        "id": "8e5b50ac"
      },
      "outputs": [],
      "source": [
        "P = ParamSpec(\"P\")\n",
        "T = TypeVar(\"T\")\n",
        "\n",
        "\n",
        "def start_end_log(logger_name=DIR_SYNC):\n",
        "    \"\"\"Create and return a decorator.\n",
        "\n",
        "    Args:\n",
        "        logger_name (str): The name of the logger to use. Defaults to DIR_SYNC.\n",
        "\n",
        "    Returns:\n",
        "        function: the real decorator\n",
        "\n",
        "    \"\"\"\n",
        "    def decorator(func: Callable[P, T]) -> Callable[P, T]:\n",
        "        \"\"\"Wrap the target function to apply the start and end log.\"\"\"\n",
        "        logger = logging.getLogger(logger_name)\n",
        "\n",
        "        @wraps(func)  # To hold the original function information\n",
        "        def wrapper(*args: P.args, **kwargs: P.kwargs) -> T:\n",
        "            \"\"\"Intercept the call and log the start and end of the execution of a function.\"\"\"\n",
        "            in_params: str = \", \".join(\n",
        "                [repr(a) for a in args] + [f\"{k}={v!r}\" for k, v in kwargs.items()]\n",
        "            )\n",
        "            logger.debug(f\"Starting {func.__name__}({in_params})\")\n",
        "            result = func(*args, **kwargs)\n",
        "            if result is not None:\n",
        "                logger.debug(f\"Finished {func.__name__} with result: {result!r}\")\n",
        "            else:\n",
        "                logger.debug(f\"Finished {func.__name__}\")\n",
        "            return result\n",
        "        return wrapper\n",
        "    return decorator"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "21533230",
      "metadata": {
        "id": "21533230"
      },
      "source": [
        "# Models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2241dbc4",
      "metadata": {
        "id": "2241dbc4"
      },
      "outputs": [],
      "source": [
        "from enum import Enum"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "868b7330",
      "metadata": {
        "id": "868b7330"
      },
      "outputs": [],
      "source": [
        "\"\"\"Defines the `ActionType` enumeration.\n",
        "\n",
        "These types are used to represent the different types of actions possible\n",
        "within the application.\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "class ActionType(str, Enum):\n",
        "    \"\"\"To represent the types of action available.\"\"\"\n",
        "\n",
        "    COPY = \"copy\"\n",
        "    DELETE = \"delete\"\n",
        "    NEW_DIR = \"new_dir\"\n",
        "    UPDATE = \"update\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1022f95f",
      "metadata": {
        "id": "1022f95f"
      },
      "outputs": [],
      "source": [
        "\"\"\"Defines the `ItemType` enumeration.\n",
        "\n",
        "These types are used to represent the different types of items possible within the application.\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "class ItemType(str, Enum):\n",
        "    \"\"\"To represent the types of item available.\"\"\"\n",
        "\n",
        "    FILE = \"file\"\n",
        "    DIRECTORY = \"directory\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "71d4d51c",
      "metadata": {
        "id": "71d4d51c"
      },
      "outputs": [],
      "source": [
        "from pathlib import Path\n",
        "\n",
        "from pydantic import BaseModel, Field"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fbbc1c88",
      "metadata": {
        "id": "fbbc1c88"
      },
      "outputs": [],
      "source": [
        "\"\"\"Data model for file information.\"\"\"\n",
        "\n",
        "\n",
        "class ItemData(BaseModel):\n",
        "    \"\"\"Holds the collected information about the items in the folders to be synchronized.\"\"\"\n",
        "\n",
        "    item_type: ItemType = Field(..., description=\"Item type: file or directory\")\n",
        "    path: Path = Field(..., description=\"Path of the item\")\n",
        "    size: int = Field(..., ge=0, description=\"The size cannot be negative\")\n",
        "    last_modification: float = Field(..., ge=0, description=\"Timestamp of last modification in float format >= 0\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f6e87e61",
      "metadata": {
        "id": "f6e87e61"
      },
      "source": [
        "# Collectors\n",
        "\n",
        "Code for collecting information about the items in a directory.  \n",
        "This is an I/O-bound type of operation, and we can use multithreading to perform it."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f9ed079f",
      "metadata": {
        "id": "f9ed079f"
      },
      "outputs": [],
      "source": [
        "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
        "from pathlib import Path"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e9290009",
      "metadata": {
        "id": "e9290009"
      },
      "outputs": [],
      "source": [
        "@start_end_log(logger_name=COLLECTOR)\n",
        "def _get_item_data(item: Path, root_path: Path) -> tuple[str, ItemData]:\n",
        "    stats = item.stat()\n",
        "    relative_path = item.relative_to(root_path)\n",
        "    if item.is_dir():\n",
        "        info = ItemData(\n",
        "            item_type=ItemType.DIRECTORY,\n",
        "            path=relative_path,\n",
        "            size=0,\n",
        "            last_modification=stats.st_mtime,\n",
        "        )\n",
        "    else:\n",
        "        info = ItemData(\n",
        "            item_type=ItemType.FILE,\n",
        "            path=relative_path,\n",
        "            size=stats.st_size,\n",
        "            last_modification=stats.st_mtime,\n",
        "        )\n",
        "    return relative_path.as_posix(), info"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bae2494c",
      "metadata": {
        "id": "bae2494c"
      },
      "outputs": [],
      "source": [
        "@start_end_log(logger_name=COLLECTOR)\n",
        "def collect_data(target: Path) -> dict[str, ItemData]:\n",
        "    \"\"\"Recursively scans a file_path and collects item data in a dictionary of FileData objects.\n",
        "\n",
        "    Args:\n",
        "        target (Path): The path to the folder to be analysed for item information.\n",
        "\n",
        "    Returns:\n",
        "        Dict[str, ItemData]: A dictionary containing all information about the items in the analysed\n",
        "         directory.\n",
        "\n",
        "    Raises:\n",
        "        Exception: If an error occurs during the collection of item information.\n",
        "\n",
        "    \"\"\"\n",
        "    items_inventory: dict[str, ItemData] = {}\n",
        "\n",
        "    items = [item for item in target.rglob(\"*\") if item.is_file() or item.is_dir()]\n",
        "    logger = logging.getLogger(COLLECTOR)\n",
        "    workers = os.cpu_count()\n",
        "    with ThreadPoolExecutor(max_workers=workers) as executor:\n",
        "        logger.info(f\"Collecting data for {len(items)} items with '{workers}' threads.\")\n",
        "        future_to_item = {executor.submit(_get_item_data, item, target): item for item in items}\n",
        "\n",
        "        for future in as_completed(future_to_item):\n",
        "            try:\n",
        "                file_path, file_data = future.result()\n",
        "                items_inventory[file_path] = file_data\n",
        "            except Exception as ex:\n",
        "                logger.error(\n",
        "                    f\"Error processing the item {future_to_item[future]}: {ex}\"\n",
        "                )\n",
        "\n",
        "    return items_inventory"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "740f552e",
      "metadata": {
        "id": "740f552e"
      },
      "source": [
        "# Operations\n",
        "\n",
        "Code for performing operations on items in a directory synchronization context."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "33a940c2",
      "metadata": {
        "id": "33a940c2"
      },
      "outputs": [],
      "source": [
        "import shutil\n",
        "from pathlib import Path\n",
        "from typing import Annotated\n",
        "\n",
        "from pydantic import AfterValidator, FilePath, validate_call"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9f9b02a8",
      "metadata": {
        "id": "9f9b02a8"
      },
      "outputs": [],
      "source": [
        "def _normalize_path(path: Path) -> Path:\n",
        "    return path.expanduser().resolve()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "46a11e3f",
      "metadata": {
        "id": "46a11e3f"
      },
      "outputs": [],
      "source": [
        "def _path_exists(exist: bool) -> AfterValidator:\n",
        "    def _check(path: Path) -> Path:\n",
        "        if exist and not path.exists():\n",
        "            raise ValueError(f\"Path does not exist: {path}\")\n",
        "        if not exist and path.exists():\n",
        "            raise ValueError(f\"Path already exist: {path}\")\n",
        "        return path\n",
        "\n",
        "    return AfterValidator(_check)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d4b0ff5a",
      "metadata": {
        "id": "d4b0ff5a"
      },
      "outputs": [],
      "source": [
        "@start_end_log()\n",
        "@validate_call\n",
        "def do_copy_update(src: FilePath, dest: Path) -> None:\n",
        "    \"\"\"Create or update operation.\n",
        "\n",
        "    Args:\n",
        "        src(Path): source path of the element to be copied or updated\n",
        "        dest(Path): destination path where the item is copied or updated\n",
        "\n",
        "    \"\"\"\n",
        "    dest.parent.mkdir(parents=True, exist_ok=True)\n",
        "    shutil.copy2(src, dest)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c09cd78d",
      "metadata": {
        "id": "c09cd78d"
      },
      "outputs": [],
      "source": [
        "@start_end_log()\n",
        "@validate_call\n",
        "def do_delete(\n",
        "    item: Annotated[Path, AfterValidator(_normalize_path), _path_exists(exist=True)],\n",
        ") -> None:\n",
        "    \"\"\"Delete file or folder.\n",
        "\n",
        "    Args:\n",
        "        item(Path): file or folder to be deleted\n",
        "\n",
        "    \"\"\"\n",
        "    if item.is_file():\n",
        "        item.unlink()\n",
        "    elif item.is_dir():\n",
        "        shutil.rmtree(item, ignore_errors=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1f243990",
      "metadata": {
        "id": "1f243990"
      },
      "outputs": [],
      "source": [
        "@start_end_log()\n",
        "@validate_call\n",
        "def do_create_dir(\n",
        "    item: Annotated[Path, AfterValidator(_normalize_path), _path_exists(exist=False)],\n",
        ") -> None:\n",
        "    \"\"\"Create folder.\n",
        "\n",
        "    Args:\n",
        "        item(Path): folder to be deleted\n",
        "\n",
        "    \"\"\"\n",
        "    item.mkdir(parents=True, exist_ok=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c0963de9",
      "metadata": {
        "id": "c0963de9"
      },
      "source": [
        "# Analyzer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "32088206",
      "metadata": {
        "id": "32088206"
      },
      "outputs": [],
      "source": [
        "import multiprocessing\n",
        "import time\n",
        "\n",
        "from pydantic import SkipValidation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f75c5ee9",
      "metadata": {
        "id": "f75c5ee9"
      },
      "outputs": [],
      "source": [
        "@validate_call\n",
        "def analyze_items(\n",
        "    path: str,\n",
        "    src: dict[str, ItemData],\n",
        "    dest: dict[str, ItemData],\n",
        "    queue: Annotated[multiprocessing.Queue, SkipValidation],\n",
        "    log_level: int = logging.ERROR,\n",
        ") -> tuple[ActionType, str] | None:\n",
        "    \"\"\"Analyzes two items to understand the type of action to take.\n",
        "\n",
        "    Possible actions are copy delete or update.\n",
        "\n",
        "    Args:\n",
        "        path (str): The relative path of the item to analyze.\n",
        "        src (dict[str, ItemData]): The source items data.\n",
        "        dest (dict[str, ItemData]): The destination items data.\n",
        "        queue (multiprocessing.Queue): The queue to use for logging from multiple processes.\n",
        "        log_level (int, optional): The logging level. Defaults to logging.ERROR.\n",
        "\n",
        "    Returns:\n",
        "        tuple[ActionType, str] | None: The action to take and the path of the item.\n",
        "\n",
        "    Raises:\n",
        "        Exception: If an error occurs during the analysis of the item.\n",
        "\n",
        "    \"\"\"\n",
        "    h = logging.handlers.QueueHandler(queue)\n",
        "    logger = logging.getLogger(ANALYZER)\n",
        "    logger.setLevel(log_level)\n",
        "    if logger.hasHandlers():\n",
        "            logger.handlers.clear()\n",
        "    logger.addHandler(h)\n",
        "\n",
        "    item_plan: tuple[ActionType, str] | None = None\n",
        "    pid = os.getpid()\n",
        "    logger.debug(f\"Worker {pid}: start analysis for the item '{path}'.\")\n",
        "    start_time = time.perf_counter()\n",
        "    try:\n",
        "        in_src = path in src\n",
        "        in_dest = path in dest\n",
        "        if in_src and not in_dest:\n",
        "            if src[path].item_type == ItemType.DIRECTORY:\n",
        "                item_plan = ActionType.NEW_DIR, path\n",
        "            else:\n",
        "                item_plan = ActionType.COPY, path\n",
        "        elif in_dest and not in_src:\n",
        "            item_plan = ActionType.DELETE, path\n",
        "        elif in_src and in_dest:\n",
        "            src_item = src[path]\n",
        "            dest_item = dest[path]\n",
        "            if src_item.item_type == ItemType.FILE and (\n",
        "                src_item.last_modification > dest_item.last_modification\n",
        "                or src_item.last_modification != dest_item.last_modification\n",
        "            ):\n",
        "                item_plan = ActionType.UPDATE, path\n",
        "    except Exception as e:\n",
        "        logger.error(f\"Worker {pid}: error analyzing item '{path}': {e}\")\n",
        "    end_time = time.perf_counter()\n",
        "    duration = end_time - start_time\n",
        "\n",
        "    logger.debug(f\"Worker {pid}: end analysis for the item '{path}'. Time: {duration:.2f} seconds.\")\n",
        "    return item_plan"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8634acd5",
      "metadata": {
        "id": "8634acd5"
      },
      "source": [
        "# Synchronizer"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "WSOHN_KjRl2B",
      "metadata": {
        "id": "WSOHN_KjRl2B"
      },
      "source": [
        "For the operations required to synchronise the items in the source and destination folders, as this is a CPU-bound operation, we use multiprocessing.\n",
        "\n",
        "For log management, in order to use a single file from multiple processes, can be used a queue and a QueueHandler."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1097f9a9",
      "metadata": {
        "id": "1097f9a9"
      },
      "outputs": [],
      "source": [
        "import threading\n",
        "from concurrent.futures import ProcessPoolExecutor\n",
        "from itertools import repeat\n",
        "from typing import Any\n",
        "\n",
        "from pydantic import DirectoryPath"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5d7e69f8",
      "metadata": {
        "id": "5d7e69f8"
      },
      "outputs": [],
      "source": [
        "class Synchronizer:\n",
        "    \"\"\"Class that performs synchronization between two folders in a parallel way.\"\"\"\n",
        "\n",
        "    @validate_call\n",
        "    def __init__(self, source: DirectoryPath, destination: DirectoryPath) -> None:\n",
        "        \"\"\"Initialize the synchronizer.\n",
        "\n",
        "        Args:\n",
        "            source(DirectoryPath): The source directory from which to synchronize.\n",
        "            destination(DirectoryPath): The source directory to be synchronized.\n",
        "\n",
        "        \"\"\"\n",
        "        self.logger = logging.getLogger(DIR_SYNC)\n",
        "        self._source: DirectoryPath = source\n",
        "        self._destination: DirectoryPath = destination\n",
        "\n",
        "    @start_end_log()\n",
        "    def create_action_plan(self) -> list[tuple[ActionType, str]]:\n",
        "        \"\"\"Compare the two folders and prepare an execution plan for parallel synchronisation.\n",
        "\n",
        "        Returns:\n",
        "            list[tuple[ActionType, str]]: The action plan.\n",
        "\n",
        "        \"\"\"\n",
        "        items_src: dict[str, ItemData] = collect_data(self._source)\n",
        "        items_dst: dict[str, ItemData] = collect_data(self._destination)\n",
        "        self.logger.info(f\"src size: {len(items_src)} items - dst size: {len(items_dst)} items\")\n",
        "        all_items: set[str] = items_src.keys() | items_dst.keys()\n",
        "        self.logger.info(f\"Total items to analyze: {len(all_items)}\")\n",
        "\n",
        "        start_time = time.perf_counter()\n",
        "        self.logger.info(f\"Manager e Listener started at {start_time:.2f} seconds. Preparing tasks ...\")\n",
        "        action_plan: list[tuple[ActionType, str]] = []\n",
        "\n",
        "        with multiprocessing.Manager() as manager:\n",
        "            queue = manager.Queue(-1)\n",
        "            lp = threading.Thread(target=self._logger_thread, args=(queue,))\n",
        "            lp.start()\n",
        "            with ProcessPoolExecutor(max_workers=os.cpu_count()) as executor:\n",
        "                size = round(len(all_items) / executor._max_workers)\n",
        "                self.logger.debug(f\"Worker number: {executor._max_workers} and chunck size: {size} ...\")\n",
        "                for result in executor.map(\n",
        "                    analyze_items,\n",
        "                    all_items,\n",
        "                    repeat(items_src),\n",
        "                    repeat(items_dst),\n",
        "                    repeat(queue),\n",
        "                    repeat(logging.DEBUG),\n",
        "                    chunksize=size,\n",
        "                ):\n",
        "                    if result is not None:\n",
        "                        action_plan.append(result)\n",
        "                        self.logger.debug(f\"Action plan updated with: {result}\")\n",
        "            queue.put_nowait(None)\n",
        "            lp.join()\n",
        "\n",
        "        end_time = time.perf_counter()\n",
        "        duration = end_time - start_time\n",
        "        self.logger.info(f\"Action plan created in {duration:.2f} seconds.\")\n",
        "        return action_plan\n",
        "\n",
        "    @start_end_log()\n",
        "    def execute(self, plan: list[tuple[ActionType, str]], *, dry_run: bool = False) -> None:\n",
        "        \"\"\"Perform the actions in the plan in parallel.\n",
        "\n",
        "        Args:\n",
        "            plan(list[tuple[ActionType, str]]): The action plan to be executed.\n",
        "                The possible action to be executed are COPY, UPDATE and DELETE\n",
        "            dry_run (bool): If True, perform a trial run without making any changes;\n",
        "                actions are logged only. Default False\n",
        "\n",
        "        \"\"\"\n",
        "        with ThreadPoolExecutor(max_workers=os.cpu_count()) as executor:\n",
        "            future_to_item = [\n",
        "                executor.submit(self._execute, action, path, dry_run=dry_run)\n",
        "                for action, path in plan\n",
        "            ]\n",
        "            for future in as_completed(future_to_item):\n",
        "                future.result()\n",
        "\n",
        "    def _execute(self, action: ActionType, path: str, *, dry_run: bool = False) -> None:\n",
        "        start = time.perf_counter()\n",
        "        match action:\n",
        "            case ActionType.COPY | ActionType.UPDATE:\n",
        "                if dry_run:\n",
        "                    self.logger.info(\n",
        "                        f\"[DRY RUN] copy from {self._source / path} to \"\n",
        "                        f\"{self._destination / path}\"\n",
        "                    )\n",
        "                else:\n",
        "                    do_copy_update(self._source / path, self._destination / path)\n",
        "\n",
        "            case ActionType.NEW_DIR:\n",
        "                if dry_run:\n",
        "                    self.logger.info(f\"[DRY RUN] create new folder {self._destination / path}\")\n",
        "                else:\n",
        "                    do_create_dir(self._destination / path)\n",
        "\n",
        "            case ActionType.DELETE:\n",
        "                if dry_run:\n",
        "                    self.logger.info(f\"[DRY RUN] delete item {self._destination / path}\")\n",
        "                else:\n",
        "                    do_delete(self._destination / path)\n",
        "\n",
        "            case _:\n",
        "                pass\n",
        "        end = time.perf_counter()\n",
        "        elapsed = end - start\n",
        "        self.logger.debug(f\"Action {action} for {path} end in: {elapsed:.3f} seconds\")\n",
        "\n",
        "    def _logger_thread(self, queue: \"multiprocessing.Queue[Any]\") -> None:\n",
        "        while True:\n",
        "            record = queue.get()\n",
        "            if record is None:  # We send this as a sentinel to tell the listener to quit.\n",
        "                break\n",
        "            logger = logging.getLogger(record.name)\n",
        "            logger.handle(record)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b2fda5e4",
      "metadata": {
        "id": "b2fda5e4"
      },
      "source": [
        "# Tests"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "acb1e485",
      "metadata": {
        "id": "acb1e485"
      },
      "outputs": [],
      "source": [
        "import tempfile\n",
        "import unittest\n",
        "\n",
        "from pydantic import ValidationError"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b6dba3c4",
      "metadata": {
        "id": "b6dba3c4"
      },
      "source": [
        "## Collectors"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cdf84944",
      "metadata": {
        "id": "cdf84944"
      },
      "outputs": [],
      "source": [
        "class TestFileInfoCollector(unittest.TestCase):\n",
        "    \"\"\"Test the item_info module.\"\"\"\n",
        "\n",
        "    def setUp(self) -> None:\n",
        "        \"\"\"Prepare a temporary folder.\"\"\"\n",
        "        self.test_dir = Path(tempfile.mkdtemp())\n",
        "\n",
        "    def tearDown(self) -> None:\n",
        "        \"\"\"Clear a temporary folder.\"\"\"\n",
        "        shutil.rmtree(self.test_dir)\n",
        "\n",
        "    def test_collects_file_information(self) -> None:\n",
        "        \"\"\"Check that the function correctly collects file info.\"\"\"\n",
        "        file1 = self.test_dir / \"file1.txt\"\n",
        "        file1.write_text(\"Test 1\")\n",
        "\n",
        "        subdir = self.test_dir / \"subdir\"\n",
        "        subdir.mkdir()\n",
        "        file2 = subdir / \"file2.txt\"\n",
        "        file2.write_text(\"Test 2\")\n",
        "\n",
        "        result = collect_data(self.test_dir)\n",
        "\n",
        "        expected = {\n",
        "            \"file1.txt\": ItemData(\n",
        "                item_type=ItemType.FILE,\n",
        "                path=file1.relative_to(self.test_dir),\n",
        "                size=file1.stat().st_size,\n",
        "                last_modification=file1.stat().st_mtime,\n",
        "            ),\n",
        "            \"subdir\": ItemData(\n",
        "                item_type=ItemType.DIRECTORY,\n",
        "                path=subdir.relative_to(self.test_dir),\n",
        "                size=0,\n",
        "                last_modification=subdir.stat().st_mtime,\n",
        "            ),\n",
        "            \"subdir/file2.txt\": ItemData(\n",
        "                item_type=ItemType.FILE,\n",
        "                path=file2.relative_to(self.test_dir),\n",
        "                size=file2.stat().st_size,\n",
        "                last_modification=file2.stat().st_mtime,\n",
        "            ),\n",
        "        }\n",
        "\n",
        "        self.assertEqual(result, expected)\n",
        "\n",
        "    def test_returns_empty_dict_for_empty_folder(self) -> None:\n",
        "        \"\"\"Check that the function returns an empty dictionary for empty folders.\"\"\"\n",
        "        result = collect_data(self.test_dir)\n",
        "        self.assertEqual(result, {})"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "095d04aa",
      "metadata": {
        "id": "095d04aa"
      },
      "source": [
        "## Models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f482fac8",
      "metadata": {
        "id": "f482fac8"
      },
      "outputs": [],
      "source": [
        "class TestFileData(unittest.TestCase):\n",
        "    \"\"\"Class for model tests that contains information on the characteristics of a file.\"\"\"\n",
        "\n",
        "    def test_size(self) -> None:\n",
        "        \"\"\"Test verifying the correctness of the dimension property.\"\"\"\n",
        "        with self.assertRaises(ValidationError):\n",
        "            ItemData(\n",
        "                item_type=ItemType.FILE,\n",
        "                path=Path(\"test.txt\"),\n",
        "                size=\"1a3\",\n",
        "                last_modification=1625309472.357246,\n",
        "            )\n",
        "\n",
        "    def test_data(self) -> None:\n",
        "        \"\"\"Test that the data are consistent.\"\"\"\n",
        "        try:\n",
        "            info = ItemData(\n",
        "                item_type=ItemType.FILE,\n",
        "                path=Path(\"test.txt\"),\n",
        "                size=1024,\n",
        "                last_modification=1625309472.357246,\n",
        "            )\n",
        "            self.assertEqual(info.size, 1024)\n",
        "        except ValidationError as e:\n",
        "            self.fail(f\"The creation of FileInfo with valid data has failed: {e}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4226c500",
      "metadata": {
        "id": "4226c500"
      },
      "source": [
        "## Synchronizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cSd95yGzX3FF",
      "metadata": {
        "id": "cSd95yGzX3FF"
      },
      "outputs": [],
      "source": [
        "import random\n",
        "from math import ceil"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "33e20a9f",
      "metadata": {
        "id": "33e20a9f"
      },
      "outputs": [],
      "source": [
        "class TestSynchronizer(unittest.TestCase):\n",
        "    \"\"\"Test the Synchronizer class.\"\"\"\n",
        "\n",
        "    def setUp(self) -> None:\n",
        "        \"\"\"Prepare a temporary folder.\"\"\"\n",
        "        self._test_src: Path = Path(tempfile.mkdtemp())\n",
        "        self._test_dst: Path = Path(tempfile.mkdtemp())\n",
        "\n",
        "    def tearDown(self) -> None:\n",
        "        \"\"\"Clear a temporary folder.\"\"\"\n",
        "        shutil.rmtree(self._test_src)\n",
        "        shutil.rmtree(self._test_dst)\n",
        "\n",
        "    def test_action_plan_small(self) -> None:\n",
        "        \"\"\"Test the action_plan method with a small number of items.\"\"\"\n",
        "        self._create_items(self._test_src, 20)\n",
        "        self._copy_items(self._test_src, self._test_dst)\n",
        "        plan = self._modify_random_items(self._test_dst)\n",
        "        sync: Synchronizer = Synchronizer(source=self._test_src, destination=self._test_dst)\n",
        "        res = sync.create_action_plan()\n",
        "        self.assertCountEqual(res, plan)\n",
        "\n",
        "    def test_execute_small(self) -> None:\n",
        "        \"\"\"Test the synchronize execution with a small number of items.\"\"\"\n",
        "        self._create_items(self._test_src, 20)\n",
        "        self._copy_items(self._test_src, self._test_dst)\n",
        "        plan = self._modify_random_items(self._test_dst)\n",
        "        sync: Synchronizer = Synchronizer(source=self._test_src, destination=self._test_dst)\n",
        "        sync.execute(plan)\n",
        "        src_collect_data = collect_data(self._test_src)\n",
        "        dest_collect_data = collect_data(self._test_dst)\n",
        "        self.assertEqual(set(src_collect_data), set(dest_collect_data))\n",
        "\n",
        "    def test_action_plan_large(self) -> None:\n",
        "        \"\"\"Test the action_plan method with a large number of items.\"\"\"\n",
        "        self._create_items(self._test_src, 200)\n",
        "        self._copy_items(self._test_src, self._test_dst)\n",
        "        plan = self._modify_random_items(self._test_dst)\n",
        "        sync: Synchronizer = Synchronizer(source=self._test_src, destination=self._test_dst)\n",
        "        res = sync.create_action_plan()\n",
        "        self.assertCountEqual(res, plan)\n",
        "\n",
        "    def test_execute_large(self) -> None:\n",
        "        \"\"\"Test the synchronize execution with a large number of items.\"\"\"\n",
        "        self._create_items(self._test_src, 200)\n",
        "        self._copy_items(self._test_src, self._test_dst)\n",
        "        plan = self._modify_random_items(self._test_dst)\n",
        "        sync: Synchronizer = Synchronizer(source=self._test_src, destination=self._test_dst)\n",
        "        sync.execute(plan)\n",
        "        src_collect_data = collect_data(self._test_src)\n",
        "        dest_collect_data = collect_data(self._test_dst)\n",
        "        self.assertEqual(set(src_collect_data), set(dest_collect_data))\n",
        "\n",
        "    def test_action_plan_with_dir(self) -> None:\n",
        "        \"\"\"Test the action_plan method with dir.\"\"\"\n",
        "        self._create_items(self._test_src, 300, 6, 3)\n",
        "        self._copy_items(self._test_src, self._test_dst)\n",
        "        plan = self._modify_random_items(self._test_dst)\n",
        "        sync: Synchronizer = Synchronizer(source=self._test_src, destination=self._test_dst)\n",
        "        res = sync.create_action_plan()\n",
        "        self.assertCountEqual(res, plan)\n",
        "\n",
        "    def test_execute_with_dir(self) -> None:\n",
        "        \"\"\"Test the synchronize execution with dir.\"\"\"\n",
        "        self._create_items(self._test_src, 300, 6, 3)\n",
        "        self._copy_items(self._test_src, self._test_dst)\n",
        "        plan = self._modify_random_items(self._test_dst)\n",
        "        sync: Synchronizer = Synchronizer(source=self._test_src, destination=self._test_dst)\n",
        "        sync.execute(plan)\n",
        "        src_collect_data = collect_data(self._test_src)\n",
        "        dest_collect_data = collect_data(self._test_dst)\n",
        "        self.assertEqual(set(src_collect_data), set(dest_collect_data))\n",
        "\n",
        "    def _copy_items(self, src: Path, dest: Path):\n",
        "        shutil.copytree(src, dest, dirs_exist_ok=True)\n",
        "\n",
        "    def _create_items(self, folder: Path, n_files: int = 30, n_dirs: int = 0, n_sub_dirs: int = 0):\n",
        "        os.makedirs(folder, exist_ok=True)\n",
        "        tot_dirs: int = n_dirs + n_sub_dirs + 1\n",
        "        files_dirs: int = n_files // tot_dirs\n",
        "\n",
        "        for i in range(tot_dirs):\n",
        "            dir_path: str = folder.resolve() if tot_dirs ==1 else os.path.join(folder, f\"dir_{i}\")\n",
        "            os.makedirs(dir_path, exist_ok=True)\n",
        "            if n_sub_dirs > 0:\n",
        "                for j in range(n_sub_dirs):\n",
        "                    sub_dir_path: str = os.path.join(dir_path, f\"sub_dir_{j}\")\n",
        "                    os.makedirs(sub_dir_path, exist_ok=True)\n",
        "                    self._create_files(sub_dir_path, files_dirs)\n",
        "            else:\n",
        "                self._create_files(dir_path, files_dirs)\n",
        "\n",
        "    def _create_files(self, folder: Path, n_files: int = 30):\n",
        "        for i in range(n_files):\n",
        "            file_name: str = f\"file_{i}.txt\"\n",
        "            file_path: str = os.path.join(folder, file_name)\n",
        "            with open(file_path, 'w') as f:\n",
        "                f.write(f\"Text {i}: {file_name}\")\n",
        "\n",
        "    def _modify_random_items(self, folder: Path):\n",
        "        files: list[str] = []\n",
        "        plan: list[ActionType, str]  = []\n",
        "        for root, dirs_list, files_list in os.walk(folder):\n",
        "            for f in files_list:\n",
        "                files.append(os.path.join(root, f))\n",
        "\n",
        "        if files:\n",
        "            files_to_remove = random.sample(files, min(ceil(len(files) / 4), len(files)))\n",
        "            for f in files_to_remove:\n",
        "                file_path = Path(f)\n",
        "                os.remove(file_path)\n",
        "                files.remove(f)\n",
        "                plan.append((ActionType.COPY, str(file_path.relative_to(folder))))\n",
        "\n",
        "        if files:\n",
        "            files_to_modify = random.sample(files, min(ceil(len(files) / 5), len(files)))\n",
        "            for f in files_to_modify:\n",
        "                file_path = Path(f)\n",
        "                with open(file_path, \"w\") as open_file:\n",
        "                    open_file.write(f\"Modified {file_path}\")\n",
        "                    plan.append((ActionType.UPDATE, str(file_path.relative_to(folder))))\n",
        "\n",
        "        return plan\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4RPy4IAwYRWd",
      "metadata": {
        "id": "4RPy4IAwYRWd"
      },
      "source": [
        "## Start Test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "37a166b5",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "37a166b5",
        "outputId": "5ed64290-7fba-4ade-9217-038179ca65b5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "test_data (__main__.TestFileData.test_data)\n",
            "Test that the data are consistent. ... ok\n",
            "test_size (__main__.TestFileData.test_size)\n",
            "Test verifying the correctness of the dimension property. ... ok\n",
            "test_collects_file_information (__main__.TestFileInfoCollector.test_collects_file_information)\n",
            "Check that the function correctly collects file info. ... ok\n",
            "test_returns_empty_dict_for_empty_folder (__main__.TestFileInfoCollector.test_returns_empty_dict_for_empty_folder)\n",
            "Check that the function returns an empty dictionary for empty folders. ... ok\n",
            "test_action_plan_large (__main__.TestSynchronizer.test_action_plan_large)\n",
            "Test the action_plan method with a large number of items. ... ok\n",
            "test_action_plan_small (__main__.TestSynchronizer.test_action_plan_small)\n",
            "Test the action_plan method with a small number of items. ... ok\n",
            "test_action_plan_with_dir (__main__.TestSynchronizer.test_action_plan_with_dir)\n",
            "Test the action_plan method with dir. ... ok\n",
            "test_execute_large (__main__.TestSynchronizer.test_execute_large)\n",
            "Test the synchronize execution with a large number of items. ... ok\n",
            "test_execute_small (__main__.TestSynchronizer.test_execute_small)\n",
            "Test the synchronize execution with a small number of items. ... ok\n",
            "test_execute_with_dir (__main__.TestSynchronizer.test_execute_with_dir)\n",
            "Test the synchronize execution with dir. ... ok\n",
            "\n",
            "----------------------------------------------------------------------\n",
            "Ran 10 tests in 4.861s\n",
            "\n",
            "OK\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<unittest.runner.TextTestResult run=10 errors=0 failures=0>"
            ]
          },
          "metadata": {},
          "execution_count": 138
        }
      ],
      "source": [
        "setup_logging(level=logging.DEBUG)\n",
        "\n",
        "test_file_data = unittest.TestLoader().loadTestsFromTestCase(TestFileData)\n",
        "test_file_info_collector = unittest.TestLoader().loadTestsFromTestCase(TestFileInfoCollector)\n",
        "test_synchronizer = unittest.TestLoader().loadTestsFromTestCase(TestSynchronizer)\n",
        "\n",
        "all_tests = unittest.TestSuite([test_file_data, test_file_info_collector, test_synchronizer])\n",
        "\n",
        "unittest.TextTestRunner(verbosity=2, stream=sys.stdout).run(all_tests)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "python-file-sync",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.14"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}